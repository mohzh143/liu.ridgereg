---
title: "Predictive modeling with ridgereg (BostonHousing)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Predictive modeling with ridgereg (BostonHousing)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
library(liu.ridgereg)
library(caret)
library(dplyr)
library(mlbench)
library(MASS)
set.seed(123)
```

# Introduction

This vignette demonstrates how to use the **`ridgereg()`** function (implemented in this package) for predictive modeling with the **BostonHousing** dataset from the `mlbench` package.

We compare three models:

1. Ordinary Linear Regression (`lm`)
2. Stepwise Linear Regression (`stepAIC`)
3. Ridge Regression (`ridgereg`) with cross-validated λ

---

# Data Preparation

```{r data-prep}
data("BostonHousing", package = "mlbench")

# Split training/testing sets
train_index <- createDataPartition(BostonHousing$medv, p = 0.8, list = FALSE)
train <- BostonHousing[train_index, ]
test  <- BostonHousing[-train_index, ]

# Features and target
y_train <- train$medv
y_test  <- test$medv
```

---

# 1. Ordinary Linear Regression

```{r lm-fit}
lm_model <- lm(medv ~ ., data = train)
summary(lm_model)

# Prediction and RMSE
pred_lm <- predict(lm_model, newdata = test)
rmse_lm <- sqrt(mean((pred_lm - y_test)^2))
rmse_lm
```

---

# 2. Stepwise Linear Regression (Forward Selection)

```{r stepwise}
step_model <- stepAIC(lm(medv ~ 1, data = train),
                      scope = list(lower = ~1, upper = ~.),
                      direction = "forward",
                      trace = FALSE)
summary(step_model)

pred_step <- predict(step_model, newdata = test)
rmse_step <- sqrt(mean((pred_step - y_test)^2))
rmse_step
```

---

# 3. Ridge Regression with Cross-Validation

We test λ = {0.01, 0.1, 1, 10, 100} and use 10-fold CV to find the best λ.

```{r ridge-cv}
lambdas <- c(0.01, 0.1, 1, 10, 100)
cv_results <- data.frame(lambda = lambdas, RMSE = NA)

for (i in seq_along(lambdas)) {
  lambda_val <- lambdas[i]
  folds <- createFolds(train$medv, k = 10, list = TRUE)
  fold_rmse <- numeric(length(folds))
  
  for (j in seq_along(folds)) {
    fold_train <- train[-folds[[j]], ]
    fold_test  <- train[folds[[j]], ]
    model <- ridgereg(medv ~ ., data = fold_train, lambda = lambda_val)
    pred  <- predict(model, newdata = fold_test)
    fold_rmse[j] <- sqrt(mean((pred - fold_test$medv)^2))
  }
  cv_results$RMSE[i] <- mean(fold_rmse)
}

cv_results
best_lambda <- cv_results$lambda[which.min(cv_results$RMSE)]
best_lambda
```

---

# 4. Final Ridge Model and Evaluation

```{r final-ridge}
ridge_final <- ridgereg(medv ~ ., data = train, lambda = best_lambda)
summary(ridge_final)

pred_ridge <- predict(ridge_final, newdata = test)
rmse_ridge <- sqrt(mean((pred_ridge - y_test)^2))
rmse_ridge
```

---

# Model Comparison

```{r compare}
compare_df <- data.frame(
  Model = c("Linear", "Stepwise", "Ridge"),
  RMSE = c(rmse_lm, rmse_step, rmse_ridge)
)
compare_df
```

---

# Ridge Regression Model Visualization (ridgereg version)

This section provides visual insights about the ridge regression model using **my own ridgereg()** implementation.

---

## 1. Coefficient Shrinkage Path

```{r ridge-coef-path, fig.width=6, fig.height=4}
set.seed(123)
lambdas <- c(0.001, 0.01, 0.1, 1, 10, 100)
coef_mat <- NULL

for (lam in lambdas) {
  model <- ridgereg(medv ~ ., data = train, lambda = lam)
  coefs <- coef(model)
  coef_mat <- rbind(coef_mat, coefs)
}
rownames(coef_mat) <- paste0("λ=", lambdas)

matplot(t(coef_mat[, -1]), type = "l", lty = 1, lwd = 2,
        xlab = "Coefficient Index", ylab = "Coefficient Value",
        main = "Ridge Coefficient Paths (ridgereg)")
legend("topright", legend = rownames(coef_mat), col = 1:6, lty = 1, cex = 0.8)
```

*Interpretation:* As λ increases, the ridge penalty shrinks coefficients toward zero, demonstrating regularization.

---

## 2. Cross-Validation Curve (10-Fold RMSE)

```{r ridge-cv-curve, fig.width=6, fig.height=4}
plot(cv_results$lambda, cv_results$RMSE, type = "b", pch = 19, col = "steelblue",
     xlab = expression(lambda), ylab = "Average RMSE (10-fold CV)",
     main = "Ridge CV Curve (ridgereg)")
points(best_lambda, min(cv_results$RMSE), col = "red", pch = 19, cex = 1.2)
text(best_lambda, min(cv_results$RMSE), labels = paste0("Best λ=", best_lambda), pos = 3, col = "red")
```

*Interpretation:* The red point marks the λ with the lowest RMSE from 10-fold cross-validation.

---

## 3. Predicted vs Actual Values

```{r ridge-pred-vs-actual, fig.width=6, fig.height=4}
plot(y_test, pred_ridge, pch = 19, col = "steelblue",
     xlab = "Actual MEDV", ylab = "Predicted MEDV",
     main = "Ridge Regression (ridgereg): Actual vs Predicted")
abline(0, 1, col = "red", lty = 2, lwd = 2)
```

*Interpretation:* The closer the blue points are to the red diagonal line, the better the model fits the data.
