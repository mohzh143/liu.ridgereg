<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Predictive Modeling with liu.ridgereg (BostonHousing)</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Predictive Modeling with liu.ridgereg
(BostonHousing)</h1>



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This vignette demonstrates how to use the
<strong><code>ridgereg()</code></strong> for a predictive modeling task.
We will compare the performance of our custom
<strong><code>ridgereg()</code></strong> function against a standard
Linear Model (LM) and a Linear Model with Forward Selection.</p>
<p>We will use the <code>caret</code> package to streamline the process
of data splitting, model training (including cross-validation), and
evaluation. The dataset for this analysis is the
<code>BostonHousing</code> dataset from the <code>mlbench</code>
package.</p>
<hr />
</div>
<div id="data-preparation" class="section level1">
<h1>Data Preparation</h1>
<p>First, we load the <code>BostonHousing</code> data. We then set a
random seed (<code>set.seed(123)</code>) to ensure that our data split
is reproducible. We partition the data using
<code>createDataPartition</code> from the caret package, allocating 80%
of the data for training and the remaining 20% for testing.</p>
<p>We also define our cross-validation strategy using
<code>trainControl</code>. We will use 10-fold cross-validation
(<code>method = &quot;cv&quot;, number = 10</code>) to tune our models.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="fu">data</span>(BostonHousing, <span class="at">package =</span> <span class="st">&quot;mlbench&quot;</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># Ensure reproducibility for the data split</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) </span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>trainIndex <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(BostonHousing<span class="sc">$</span>medv, <span class="at">p =</span> <span class="fl">0.8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># Create training and testing sets</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>trainData <span class="ot">&lt;-</span> BostonHousing[trainIndex, ]</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>testData  <span class="ot">&lt;-</span> BostonHousing[<span class="sc">-</span>trainIndex, ]</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Training data rows:&quot;</span>, <span class="fu">nrow</span>(trainData), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Training data rows: 407</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Testing data rows:&quot;</span>, <span class="fu">nrow</span>(testData), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Testing data rows: 99</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Set up 10-fold cross-validation</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>trControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>,    <span class="co"># Cross-Validation</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>  <span class="at">number =</span> <span class="dv">10</span>       <span class="co"># 10 folds</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>)</span></code></pre></div>
<hr />
</div>
<div id="fit-linear-models" class="section level1">
<h1>Fit Linear Models</h1>
<div id="standard-linear-regression-lm" class="section level2">
<h2>Standard Linear Regression (LM)</h2>
<p>We fit a standard linear regression model using all available
predictors. Because ridge regression is sensitive to the scale of
predictors, we must standardize (center and scale) our predictors here
for a fair comparison. caret handles this via the preProcess
argument.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Ensure reproducibility for CV folds</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) </span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>model_lm <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>  medv <span class="sc">~</span> .,</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>  <span class="at">data =</span> trainData,</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>  <span class="at">trControl =</span> trControl,</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>  <span class="at">preProcess =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>) <span class="co"># Standardize predictors</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>)</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a><span class="co"># Print the CV results</span></span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a><span class="fu">print</span>(model_lm)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 407 samples
##  13 predictor
## 
## Pre-processing: centered (13), scaled (13) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 366, 367, 366, 366, 366, 366, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE   
##   4.818679  0.7360525  3.4088
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
</div>
<div id="linear-regression-with-forward-selection" class="section level2">
<h2>Linear Regression with Forward Selection</h2>
<p>Next, we fit a linear model using forward selection
(<code>method = &quot;leapForward&quot;</code>). This method treats the number of
predictors (<code>nvmax</code>) as a tuning parameter. We create a
<code>tuneGrid</code> to test all possible numbers of predictors, from 1
to 13 (the total number of predictors for this case).</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># Define the tuning grid: test models from 1 to p variables</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(trainData) <span class="sc">-</span> <span class="dv">1</span> <span class="co"># p = 13 predictors</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>fwd_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">nvmax =</span> <span class="dv">1</span><span class="sc">:</span>p)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="co"># Ensure reproducibility</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) </span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>model_fwd <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>  medv <span class="sc">~</span> .,</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>  <span class="at">data =</span> trainData,</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;leapForward&quot;</span>,</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>  <span class="at">trControl =</span> trControl,</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>  <span class="at">preProcess =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>), <span class="co"># Standardize predictors</span></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>  <span class="at">tuneGrid =</span> fwd_grid               <span class="co"># Tune the number of variables</span></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>)</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a><span class="fu">print</span>(model_fwd)</span></code></pre></div>
<pre><code>## Linear Regression with Forward Selection 
## 
## 407 samples
##  13 predictor
## 
## Pre-processing: centered (13), scaled (13) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 366, 367, 366, 366, 366, 366, ... 
## Resampling results across tuning parameters:
## 
##   nvmax  RMSE      Rsquared   MAE     
##    1     6.078799  0.5988185  4.476019
##    2     5.535976  0.6729081  4.005710
##    3     5.213052  0.6945262  3.682000
##    4     5.216457  0.6929382  3.674384
##    5     5.099042  0.7061672  3.585318
##    6     5.009519  0.7159313  3.481756
##    7     5.002583  0.7150297  3.467237
##    8     4.940668  0.7220457  3.463943
##    9     4.923702  0.7244597  3.440252
##   10     4.888873  0.7286459  3.439640
##   11     4.807413  0.7375883  3.390684
##   12     4.815615  0.7364321  3.402883
##   13     4.818679  0.7360525  3.408800
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was nvmax = 11.</code></pre>
<p>We can plot the cross-validation error (RMSE) as a function of the
number of variables included in the model.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="fu">plot</span>(model_fwd, <span class="at">main =</span> <span class="st">&quot;RMSE vs. Number of Variables&quot;</span>,<span class="at">cex.main =</span> <span class="fl">0.1</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" width="70%" style="display: block; margin: auto;" /></p>
<p>As the figure shown above, the cross-validated RMSE consistently
decreases as we add more variables, dropping sharply from 6.08 (1
variable) to 5.01 (6 variables). The optimal model, as determined by the
lowest RMSE, is the one with nvmax = 11 variables, which achieved an
RMSE of 4.807. Adding the 12th and 13th variables causes the RMSE to
slightly increase (to 4.816 and 4.819, respectively). This value for
nvmax = 13 (4.819) correctly matches the RMSE of our standard linear
model (model_lm), which uses all 13 predictors. This slight increase
suggests that the full model is marginally overfit, and a 11-variable
model provides slightly better predictive accuracy on this training
data.</p>
<p>We can also extract the variables selected by the best-performing
model.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>best_nvmax <span class="ot">&lt;-</span> model_fwd<span class="sc">$</span>bestTune<span class="sc">$</span>nvmax</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="co"># Get coefficients from the final model (which is a &#39;regsubsets&#39; object)</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>best_coefs <span class="ot">&lt;-</span> <span class="fu">coef</span>(model_fwd<span class="sc">$</span>finalModel, <span class="at">id =</span> best_nvmax)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Selected variables:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Selected variables:</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">names</span>(best_coefs))</span></code></pre></div>
<pre><code>##  [1] &quot;(Intercept)&quot; &quot;crim&quot;        &quot;zn&quot;          &quot;chas1&quot;       &quot;nox&quot;        
##  [6] &quot;rm&quot;          &quot;dis&quot;         &quot;rad&quot;         &quot;tax&quot;         &quot;ptratio&quot;    
## [11] &quot;b&quot;           &quot;lstat&quot;</code></pre>
<hr />
</div>
</div>
<div id="evaluate-performance-on-training-set" class="section level1">
<h1>Evaluate Performance on Training Set</h1>
<p>We can now compare the cross-validation performance of above two
models. The <code>resamples</code> function collects the CV results from
models trained with the same resampling indices, allowing for a direct
comparison.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>models_list_base <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>  <span class="at">LM =</span> model_lm,</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>  <span class="at">Forward =</span> model_fwd</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>)</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>cv_results_base <span class="ot">&lt;-</span> <span class="fu">resamples</span>(models_list_base)</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="co"># Show summary of CV statistics</span></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="fu">summary</span>(cv_results_base)</span></code></pre></div>
<pre><code>## 
## Call:
## summary.resamples(object = cv_results_base)
## 
## Models: LM, Forward 
## Number of resamples: 10 
## 
## MAE 
##             Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## LM      2.740893 3.005405 3.404101 3.408800 3.792326 4.237117    0
## Forward 2.713556 2.965090 3.390514 3.390684 3.789517 4.244066    0
## 
## RMSE 
##             Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## LM      3.652840 4.164296 4.697268 4.818679 5.409228 6.154202    0
## Forward 3.617192 4.151341 4.696125 4.807413 5.385284 6.157500    0
## 
## Rsquared 
##              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## LM      0.5949196 0.7079916 0.7707270 0.7360525 0.7902890 0.8119818    0
## Forward 0.5979866 0.7074637 0.7710428 0.7375883 0.7915799 0.8153254    0</code></pre>
<p>The <code>summary(cv_results_base)</code> table provides the
numerical details for our 10-fold cross-validation. The full Linear
Model (LM) had a mean CV RMSE of 4.819, while the 11-variable Forward
Selection model had a mean CV RMSE of 4.807.</p>
<p>A boxplot provides a clear visual comparison of the distribution of
RMSE values from the 10-fold cross-validation for each model.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="fu">bwplot</span>(cv_results_base, </span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>       <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>       <span class="at">cex =</span> <span class="fl">0.7</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" width="70%" style="display: block; margin: auto;" />
Above boxplot visually confirms our finding. The entire distribution of
cross-validation errors for the ‘Forward’ model—including its median
(black dot) and interquartile range (the box)—is shifted slightly to the
left (indicating a lower RMSE) compared to the ‘LM’. While the
performance difference is very small (approx. 0.012), it reinforces our
earlier conclusion: the 11-variable model selected by forward selection
is slightly more robust and less overfit than the full 13-variable
model.</p>
<hr />
</div>
<div id="fit-and-tune-ridge-regression" class="section level1">
<h1>Fit and Tune Ridge Regression</h1>
<p>Now we fit our custom <code>ridgereg</code> model using the caret
interface we defined in <code>myRidgeModel</code>. We will tune the
<code>lambda</code> hyperparameter. We create a dense grid of 100
<code>lambda</code> values, ranging logarithmically from <span class="math inline">\(10^{-4}\)</span> to <span class="math inline">\(10^{2}\)</span> (0.0001 to 100). Important: We do
not use the <code>preProcess</code> argument here, because our ridgereg
function performs its own internal standardization, as required by the
assignment.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="co"># Create a dense grid of lambda values</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>lambda_grid_dense <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>  <span class="at">lambda =</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">2</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>)</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">paste</span>(<span class="st">&quot;Tuning ridge regression with&quot;</span>, <span class="fu">nrow</span>(lambda_grid_dense), <span class="st">&quot;lambda values.</span><span class="sc">\n</span><span class="st">&quot;</span>))</span></code></pre></div>
<pre><code>## Tuning ridge regression with 100 lambda values.</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">paste</span>(<span class="st">&quot;Lambda range:&quot;</span>, </span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>          <span class="fu">format</span>(<span class="fu">min</span>(lambda_grid_dense<span class="sc">$</span>lambda), <span class="at">scientific =</span> <span class="cn">FALSE</span>), </span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>          <span class="st">&quot;to&quot;</span>, </span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>          <span class="fu">max</span>(lambda_grid_dense<span class="sc">$</span>lambda), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span></code></pre></div>
<pre><code>## Lambda range: 0.0001 to 100</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="co"># Ensure reproducibility</span></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>model_ridge <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(</span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>  medv <span class="sc">~</span> .,</span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>  <span class="at">data =</span> trainData,</span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a>  <span class="at">method =</span> myRidgeModel,  <span class="co"># Our custom model interface</span></span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a>  <span class="at">trControl =</span> trControl,</span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a>  <span class="at">tuneGrid =</span> lambda_grid_dense</span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a>)</span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb22-12"><a href="#cb22-12" tabindex="-1"></a><span class="fu">print</span>(model_ridge)</span></code></pre></div>
<pre><code>## 407 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 366, 367, 366, 366, 366, 366, ... 
## Resampling results across tuning parameters:
## 
##   lambda        RMSE      Rsquared   MAE     
##   1.000000e-04  4.818679  0.7360525  3.408799
##   1.149757e-04  4.818679  0.7360525  3.408799
##   1.321941e-04  4.818679  0.7360526  3.408799
##   1.519911e-04  4.818679  0.7360526  3.408799
##   1.747528e-04  4.818679  0.7360526  3.408798
##   2.009233e-04  4.818679  0.7360526  3.408798
##   2.310130e-04  4.818679  0.7360526  3.408798
##   2.656088e-04  4.818679  0.7360526  3.408798
##   3.053856e-04  4.818679  0.7360526  3.408797
##   3.511192e-04  4.818679  0.7360526  3.408797
##   4.037017e-04  4.818679  0.7360526  3.408797
##   4.641589e-04  4.818678  0.7360526  3.408796
##   5.336699e-04  4.818678  0.7360526  3.408796
##   6.135907e-04  4.818678  0.7360526  3.408795
##   7.054802e-04  4.818678  0.7360526  3.408794
##   8.111308e-04  4.818678  0.7360526  3.408793
##   9.326033e-04  4.818678  0.7360526  3.408792
##   1.072267e-03  4.818677  0.7360526  3.408791
##   1.232847e-03  4.818677  0.7360527  3.408790
##   1.417474e-03  4.818677  0.7360527  3.408789
##   1.629751e-03  4.818676  0.7360527  3.408787
##   1.873817e-03  4.818676  0.7360527  3.408785
##   2.154435e-03  4.818675  0.7360527  3.408783
##   2.477076e-03  4.818675  0.7360528  3.408781
##   2.848036e-03  4.818674  0.7360528  3.408778
##   3.274549e-03  4.818673  0.7360529  3.408774
##   3.764936e-03  4.818672  0.7360529  3.408771
##   4.328761e-03  4.818671  0.7360530  3.408766
##   4.977024e-03  4.818670  0.7360530  3.408761
##   5.722368e-03  4.818669  0.7360531  3.408756
##   6.579332e-03  4.818667  0.7360532  3.408749
##   7.564633e-03  4.818665  0.7360533  3.408741
##   8.697490e-03  4.818663  0.7360534  3.408733
##   1.000000e-02  4.818661  0.7360535  3.408722
##   1.149757e-02  4.818658  0.7360536  3.408711
##   1.321941e-02  4.818655  0.7360538  3.408698
##   1.519911e-02  4.818652  0.7360540  3.408682
##   1.747528e-02  4.818647  0.7360542  3.408665
##   2.009233e-02  4.818643  0.7360545  3.408645
##   2.310130e-02  4.818637  0.7360548  3.408621
##   2.656088e-02  4.818631  0.7360551  3.408595
##   3.053856e-02  4.818624  0.7360555  3.408564
##   3.511192e-02  4.818615  0.7360559  3.408529
##   4.037017e-02  4.818606  0.7360564  3.408488
##   4.641589e-02  4.818595  0.7360570  3.408442
##   5.336699e-02  4.818583  0.7360576  3.408388
##   6.135907e-02  4.818568  0.7360583  3.408327
##   7.054802e-02  4.818552  0.7360592  3.408256
##   8.111308e-02  4.818533  0.7360602  3.408175
##   9.326033e-02  4.818511  0.7360613  3.408081
##   1.072267e-01  4.818487  0.7360626  3.407974
##   1.232847e-01  4.818459  0.7360640  3.407851
##   1.417474e-01  4.818426  0.7360656  3.407709
##   1.629751e-01  4.818390  0.7360675  3.407546
##   1.873817e-01  4.818348  0.7360696  3.407359
##   2.154435e-01  4.818300  0.7360720  3.407145
##   2.477076e-01  4.818246  0.7360747  3.406898
##   2.848036e-01  4.818184  0.7360777  3.406615
##   3.274549e-01  4.818114  0.7360811  3.406291
##   3.764936e-01  4.818036  0.7360849  3.405918
##   4.328761e-01  4.817947  0.7360891  3.405491
##   4.977024e-01  4.817847  0.7360937  3.405002
##   5.722368e-01  4.817735  0.7360987  3.404440
##   6.579332e-01  4.817611  0.7361042  3.403797
##   7.564633e-01  4.817473  0.7361100  3.403061
##   8.697490e-01  4.817322  0.7361161  3.402219
##   1.000000e+00  4.817156  0.7361224  3.401256
##   1.149757e+00  4.816976  0.7361286  3.400156
##   1.321941e+00  4.816784  0.7361346  3.398900
##   1.519911e+00  4.816581  0.7361398  3.397480
##   1.747528e+00  4.816371  0.7361439  3.395944
##   2.009233e+00  4.816158  0.7361462  3.394259
##   2.310130e+00  4.815950  0.7361456  3.392345
##   2.656088e+00  4.815757  0.7361411  3.390207
##   3.053856e+00  4.815590  0.7361312  3.387865
##   3.511192e+00  4.815467  0.7361140  3.385331
##   4.037017e+00  4.815408  0.7360874  3.382568
##   4.641589e+00  4.815440  0.7360485  3.379530
##   5.336699e+00  4.815595  0.7359941  3.376423
##   6.135907e+00  4.815911  0.7359205  3.373225
##   7.054802e+00  4.816432  0.7358233  3.370115
##   8.111308e+00  4.817212  0.7356976  3.367190
##   9.326033e+00  4.818309  0.7355378  3.363941
##   1.072267e+01  4.819790  0.7353381  3.360407
##   1.232847e+01  4.821732  0.7350920  3.356487
##   1.417474e+01  4.824215  0.7347927  3.352652
##   1.629751e+01  4.827333  0.7344330  3.349359
##   1.873817e+01  4.831187  0.7340053  3.346986
##   2.154435e+01  4.835888  0.7335018  3.345911
##   2.477076e+01  4.841563  0.7329142  3.345304
##   2.848036e+01  4.848354  0.7322335  3.345216
##   3.274549e+01  4.856422  0.7314501  3.346025
##   3.764936e+01  4.865954  0.7305533  3.346809
##   4.328761e+01  4.877166  0.7295310  3.348894
##   4.977024e+01  4.890309  0.7283693  3.353634
##   5.722368e+01  4.905673  0.7270520  3.360018
##   6.579332e+01  4.923594  0.7255599  3.367735
##   7.564633e+01  4.944457  0.7238703  3.378978
##   8.697490e+01  4.968696  0.7219564  3.393092
##   1.000000e+02  4.996796  0.7197867  3.411633
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was lambda = 4.037017.</code></pre>
<p>We can plot the RMSE across the range of <code>lambda</code> values
(on a log scale) to find the optimal tuning parameter.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="fu">plot</span>(model_ridge, <span class="at">main =</span> <span class="st">&quot;RMSE vs. log(Lambda)&quot;</span>,<span class="at">cex.main =</span> <span class="fl">0.1</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" width="70%" style="display: block; margin: auto;" />
The best lambda value found during cross-validation is:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">paste</span>(<span class="st">&quot;Best lambda:&quot;</span>, model_ridge<span class="sc">$</span>bestTune<span class="sc">$</span>lambda, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span></code></pre></div>
<pre><code>## Best lambda: 4.03701725859656</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a>best_rmse_ridge <span class="ot">&lt;-</span> <span class="fu">min</span>(model_ridge<span class="sc">$</span>results<span class="sc">$</span>RMSE)</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">paste</span>(<span class="st">&quot;Corresponding RMSE:&quot;</span>, best_rmse_ridge, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span></code></pre></div>
<pre><code>## Corresponding RMSE: 4.81540823736812</code></pre>
<p>The output table and the RMSE vs. log(Lambda) plot reveal a ridge
tuning pattern:<br />
–When <code>lambda</code> is extremely small (e.g., <span class="math inline">\(10^{-4}\)</span>), the RMSE is 4.818679, which is
identical to the standard Linear Model (model_lm). This is the correct
behavior, as a penalty of (near) zero makes ridge regression equivalent
to OLS.<br />
–As the penalty <code>lambda</code> increases, the CV RMSE slightly
decreases. This indicates that applying a small amount of shrinkage
(penalizing large coefficients) improves the model’s predictive
performance on unseen data by reducing variance.<br />
–The optimal model was found at <code>lambda = 4.037</code>, which
achieved the minimum CV RMSE of 4.815.<br />
–As <code>lambda</code> continues to increase past this optimal point,
the plot shows the RMSE rising again (e.g., 4.997 at lambda=100). This
indicates underfitting, where the penalty is too strong and introduces
excessive bias into the model.</p>
<hr />
</div>
<div id="final-model-evaluation-on-the-test-set" class="section level1">
<h1>Final Model Evaluation on the Test Set</h1>
<p>We use the <code>testData</code> set, which the models have never
seen before, to get an unbiased estimate of their real-world
performance. We create a summary table comparing the Cross-Validation
RMSE (from the training set) to the final RMSE on the test set.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="co"># 1. Make predictions on the test set</span></span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a>pred_lm <span class="ot">&lt;-</span> <span class="fu">predict</span>(model_lm, testData)</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>pred_fwd <span class="ot">&lt;-</span> <span class="fu">predict</span>(model_fwd, testData)</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>pred_ridge <span class="ot">&lt;-</span> <span class="fu">predict</span>(model_ridge, testData)</span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a><span class="co"># 2. Get performance metrics on test set</span></span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a><span class="co"># postResample gives RMSE, Rsquared, and MAE</span></span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a>res_lm <span class="ot">&lt;-</span> <span class="fu">postResample</span>(<span class="at">pred =</span> pred_lm, <span class="at">obs =</span> testData<span class="sc">$</span>medv)</span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a>res_fwd <span class="ot">&lt;-</span> <span class="fu">postResample</span>(<span class="at">pred =</span> pred_fwd, <span class="at">obs =</span> testData<span class="sc">$</span>medv)</span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a>res_ridge <span class="ot">&lt;-</span> <span class="fu">postResample</span>(<span class="at">pred =</span> pred_ridge, <span class="at">obs =</span> testData<span class="sc">$</span>medv)</span>
<span id="cb29-11"><a href="#cb29-11" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" tabindex="-1"></a><span class="co"># 3. Get the best CV RMSE from our previous models</span></span>
<span id="cb29-13"><a href="#cb29-13" tabindex="-1"></a>cv_rmse_lm <span class="ot">&lt;-</span> model_lm<span class="sc">$</span>results<span class="sc">$</span>RMSE</span>
<span id="cb29-14"><a href="#cb29-14" tabindex="-1"></a>cv_rmse_fwd <span class="ot">&lt;-</span> <span class="fu">min</span>(model_fwd<span class="sc">$</span>results<span class="sc">$</span>RMSE)</span>
<span id="cb29-15"><a href="#cb29-15" tabindex="-1"></a>cv_rmse_ridge <span class="ot">&lt;-</span> <span class="fu">min</span>(model_ridge<span class="sc">$</span>results<span class="sc">$</span>RMSE)</span>
<span id="cb29-16"><a href="#cb29-16" tabindex="-1"></a></span>
<span id="cb29-17"><a href="#cb29-17" tabindex="-1"></a><span class="co"># 4. Combine into a summary data.frame</span></span>
<span id="cb29-18"><a href="#cb29-18" tabindex="-1"></a>summary_table <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb29-19"><a href="#cb29-19" tabindex="-1"></a>  <span class="st">&quot;CV RMSE (Train)&quot;</span> <span class="ot">=</span> <span class="fu">c</span>(cv_rmse_lm, cv_rmse_fwd, cv_rmse_ridge),</span>
<span id="cb29-20"><a href="#cb29-20" tabindex="-1"></a>  <span class="st">&quot;Test RMSE&quot;</span> <span class="ot">=</span> <span class="fu">c</span>(res_lm[<span class="st">&quot;RMSE&quot;</span>], res_fwd[<span class="st">&quot;RMSE&quot;</span>], res_ridge[<span class="st">&quot;RMSE&quot;</span>]),</span>
<span id="cb29-21"><a href="#cb29-21" tabindex="-1"></a>  <span class="st">&quot;Test Rsquared&quot;</span> <span class="ot">=</span> <span class="fu">c</span>(res_lm[<span class="st">&quot;Rsquared&quot;</span>], res_fwd[<span class="st">&quot;Rsquared&quot;</span>], res_ridge[<span class="st">&quot;Rsquared&quot;</span>])</span>
<span id="cb29-22"><a href="#cb29-22" tabindex="-1"></a>)</span>
<span id="cb29-23"><a href="#cb29-23" tabindex="-1"></a><span class="fu">rownames</span>(summary_table) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Linear Model (LM)&quot;</span>, <span class="st">&quot;Forward Selection&quot;</span>, <span class="st">&quot;Ridge (liu.ridgereg)&quot;</span>)</span>
<span id="cb29-24"><a href="#cb29-24" tabindex="-1"></a></span>
<span id="cb29-25"><a href="#cb29-25" tabindex="-1"></a><span class="co"># 5. Print the final comparison table</span></span>
<span id="cb29-26"><a href="#cb29-26" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Final Model Performance Comparison:</span><span class="sc">\n\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Final Model Performance Comparison:</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="fu">print</span>(summary_table, <span class="at">digits =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##                      CV.RMSE..Train. Test.RMSE Test.Rsquared
## Linear Model (LM)              4.819     4.589        0.7611
## Forward Selection              4.807     4.561        0.7646
## Ridge (liu.ridgereg)           4.815     4.588        0.7628</code></pre>
<p><strong>Concluding Comments:</strong><br />
–<strong>Final Rankings</strong>: The rankings on the test set
are:<br />
1st: Forward Selection (Test RMSE = 4.561)<br />
2nd: Ridge (liu.ridgereg) (Test RMSE = 4.588)<br />
3rd: Linear Model (LM) (Test RMSE = 4.589)<br />
–<strong>Conclusion</strong>: The Forward Selection model has the best
performance in cross-validation (CV RMSE 4.807) and this translated into
the best performance on the final test set (Test RMSE 4.561).<br />
–<strong>ridgereg Performance</strong>: Our custom ridgereg function
performed as intended. Its Test RMSE of 4.588 was an improvement,
however slight, over the standard Linear Model’s 4.589. This
demonstrates that applying a small shrinkage penalty (lambda = 4.037)
provided a small but real regularization benefit, improving the model’s
ability to generalize.<br />
–<strong>CV vs. Test</strong>: All three models performed better on the
test set than their average CV RMSE from the training set. This suggests
our 20% test split was, by chance, slightly easier to predict than the
training folds. The most important finding is that the relative ranking
of the models (Forward &gt; Ridge &gt; LM) remained consistent between
training and testing, giving us high confidence in our model selection
process.<br />
In summary, while the performance differences are small, our analysis
shows that regularization (both via variable selection and L2 shrinkage)
provided a measurable benefit for this dataset.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
